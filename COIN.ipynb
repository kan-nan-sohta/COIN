{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lined-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "satellite-investor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# device = torch.device('cuda:0'\n",
    "#                       if torch.cuda.is_available()\n",
    "#                       else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(device)\n",
    "print(torch.__version__)#1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alpine-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = 'img/cake.jpg'\n",
    "BATCH_SIZE = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "strong-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image(Dataset):\n",
    "    def __init__(self, img_path):\n",
    "        img = plt.imread(img_path)\n",
    "        if img.shape[2] == 4:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)\n",
    "        if np.max(img) <= 1:\n",
    "            img *= 255\n",
    "            img = img.astype(np.uint8)\n",
    "        h, w, c = img.shape\n",
    "        img = cv2.resize(img, (500, int(500*h/w)))\n",
    "        h, w, c = img.shape\n",
    "        img = img.astype(np.float)\n",
    "        img /= np.max(img)\n",
    "        yx = np.mgrid[0:1:1./h, 0:1:1./w].reshape(2, -1).T\n",
    "        color = img.reshape(-1, 3)\n",
    "        self.data = torch.from_numpy(yx).to(device)\n",
    "        self.t = torch.from_numpy(color).to(device)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.t[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cleared-parking",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/kawanan/anaconda3/envs/COIN/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "dataset = Image(IMG_PATH)\n",
    "batch_size = BATCH_SIZE\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle = True)\n",
    "loop = len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "polar-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "continued-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, activation=lambda x: x):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = nn.Parameter(\n",
    "            torch.Tensor(\n",
    "                np.random.normal(\n",
    "                    size=(input_dim, output_dim)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.b = nn.Parameter(\n",
    "            torch.Tensor(\n",
    "                np.zeros(output_dim)\n",
    "            )\n",
    "        )\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.device, self.W.device, self.b.device)\n",
    "        return self.activation(torch.matmul(x, self.W) + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "second-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(x):\n",
    "    return x * torch.tanh(F.softplus(x))\n",
    "class YX2COLOR(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        self.layer1 = Dense(input_dim, hidden_dim, activation= torch.tanh)\n",
    "        self.layer2 = Dense(hidden_dim, hidden_dim, activation=torch.tanh)\n",
    "        self.layer3 = Dense(hidden_dim, hidden_dim, activation=torch.tanh)\n",
    "        self.layer4 = Dense(hidden_dim, hidden_dim, activation=torch.tanh)\n",
    "        self.layer5 = Dense(hidden_dim, output_dim, activation=torch.sigmoid)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.layer1(x)\n",
    "        output = self.layer2(output)\n",
    "        output = self.layer3(output)\n",
    "        output = self.layer4(output)\n",
    "        output = self.layer5(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "asian-lightweight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default image(H*W*C):\t 562500\n",
      "model param count:\t 5937\n"
     ]
    }
   ],
   "source": [
    "print('default image(H*W*C):\\t', len(dataset)*3)\n",
    "hidden_dim = int((len(dataset)/100)**0.5)\n",
    "model = YX2COLOR(2, 3, hidden_dim=hidden_dim).to(device)\n",
    "#if torch.cuda.is_available():\n",
    "#    model = torch.nn.DataParallel(model) \n",
    "params = 0\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        params += p.numel()\n",
    "print('model param count:\\t', params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "several-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "lr = 1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "def compute_loss(t, y):\n",
    "    return criterion(t, y)\n",
    "\n",
    "def train_step(x, t):\n",
    "    model.train()\n",
    "    y = model(x)\n",
    "    loss = compute_loss(t, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-ministry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97%\n",
      "epoch: 1, loss: 7.34\n",
      " 97%\n",
      "epoch: 2, loss: 5.36\n",
      " 97%\n",
      "epoch: 3, loss: 4.16\n",
      " 97%\n",
      "epoch: 4, loss: 3.28\n",
      " 97%\n",
      "epoch: 5, loss: 3.04\n",
      " 97%\n",
      "epoch: 6, loss: 2.88\n",
      " 97%\n",
      "epoch: 7, loss: 2.78\n",
      " 97%\n",
      "epoch: 8, loss: 2.69\n",
      " 97%\n",
      "epoch: 9, loss: 2.6\n",
      " 97%\n",
      "epoch: 10, loss: 2.5\n",
      " 97%\n",
      "epoch: 11, loss: 2.42\n",
      " 97%\n",
      "epoch: 12, loss: 2.38\n",
      " 97%\n",
      "epoch: 13, loss: 2.34\n",
      " 97%\n",
      "epoch: 14, loss: 2.31\n",
      " 97%\n",
      "epoch: 15, loss: 2.27\n",
      " 97%\n",
      "epoch: 16, loss: 2.28\n",
      " 97%\n",
      "epoch: 17, loss: 2.25\n",
      " 97%\n",
      "epoch: 18, loss: 2.21\n",
      " 97%\n",
      "epoch: 19, loss: 2.2\n",
      " 97%\n",
      "epoch: 20, loss: 2.16\n",
      " 97%\n",
      "epoch: 21, loss: 2.15\n",
      " 97%\n",
      "epoch: 22, loss: 2.15\n",
      " 97%\n",
      "epoch: 23, loss: 2.13\n",
      " 97%\n",
      "epoch: 24, loss: 2.11\n",
      " 97%\n",
      "epoch: 25, loss: 2.09\n",
      " 97%\n",
      "epoch: 26, loss: 2.09\n",
      " 97%\n",
      "epoch: 27, loss: 2.07\n",
      " 97%\n",
      "epoch: 28, loss: 2.06\n",
      " 97%\n",
      "epoch: 29, loss: 2.06\n",
      " 97%\n",
      "epoch: 30, loss: 2.05\n",
      " 97%\n",
      "epoch: 31, loss: 2.03\n",
      " 97%\n",
      "epoch: 32, loss: 2.03\n",
      " 97%\n",
      "epoch: 33, loss: 2.02\n",
      " 97%\n",
      "epoch: 34, loss: 2.0\n",
      " 97%\n",
      "epoch: 35, loss: 2.0\n",
      " 97%\n",
      "epoch: 36, loss: 1.98\n",
      " 97%\n",
      "epoch: 37, loss: 1.98\n",
      " 97%\n",
      "epoch: 38, loss: 1.96\n",
      " 97%\n",
      "epoch: 39, loss: 1.95\n",
      " 97%\n",
      "epoch: 40, loss: 1.95\n",
      " 97%\n",
      "epoch: 41, loss: 1.94\n",
      " 97%\n",
      "epoch: 42, loss: 1.94\n",
      " 97%\n",
      "epoch: 43, loss: 1.93\n",
      " 97%\n",
      "epoch: 44, loss: 1.92\n",
      " 97%\n",
      "epoch: 45, loss: 1.91\n",
      " 97%\n",
      "epoch: 46, loss: 1.9\n",
      " 97%\n",
      "epoch: 47, loss: 1.89\n",
      " 97%\n",
      "epoch: 48, loss: 1.88\n",
      " 97%\n",
      "epoch: 49, loss: 1.89\n",
      " 97%\n",
      "epoch: 50, loss: 1.87\n",
      " 97%\n",
      "epoch: 51, loss: 1.85\n",
      " 97%\n",
      "epoch: 52, loss: 1.86\n",
      " 97%\n",
      "epoch: 53, loss: 1.84\n",
      " 97%\n",
      "epoch: 54, loss: 1.84\n",
      " 97%52%\n",
      "epoch: 55, loss: 1.85\n",
      " 97%\n",
      "epoch: 56, loss: 1.83\n",
      " 97%\n",
      "epoch: 57, loss: 1.82\n",
      " 97%\n",
      "epoch: 58, loss: 1.8\n",
      " 97%\n",
      "epoch: 59, loss: 1.82\n",
      " 97%\n",
      "epoch: 60, loss: 1.8\n",
      " 97%\n",
      "epoch: 61, loss: 1.79\n",
      " 97%\n",
      "epoch: 62, loss: 1.79\n",
      " 97%\n",
      "epoch: 63, loss: 1.78\n",
      " 97%\n",
      "epoch: 64, loss: 1.78\n",
      " 97%\n",
      "epoch: 65, loss: 1.77\n",
      " 97%\n",
      "epoch: 66, loss: 1.77\n",
      " 97%\n",
      "epoch: 67, loss: 1.76\n",
      " 97%\n",
      "epoch: 68, loss: 1.75\n",
      " 97%\n",
      "epoch: 69, loss: 1.75\n",
      " 97%\n",
      "epoch: 70, loss: 1.74\n",
      " 97%\n",
      "epoch: 71, loss: 1.74\n",
      " 21%"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.\n",
    "    \n",
    "    for i, (X_batch, y_batch) in enumerate(dataloader):\n",
    "        print('\\r', str(int(i*100/loop))+'%', end='')\n",
    "        loss, y = train_step(X_batch.float(), y_batch.float())\n",
    "        train_loss += loss.item()\n",
    "    print()\n",
    "        \n",
    "    if epoch % 1 == 0 or epoch == epochs - 1:\n",
    "        print('epoch: {}, loss: {:.3}'.format(\n",
    "            epoch + 1,\n",
    "            train_loss,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "YX = torch.from_numpy(\n",
    "    np.mgrid[0:1:1./1000, 0:1:1./1000].reshape(2, -1).T\n",
    ").float().to(device)\n",
    "\n",
    "s = model(YX).to('cpu').detach().numpy()\n",
    "print(s.shape)\n",
    "s = s.reshape(1000, 1000, 3)\n",
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = plt.imread(IMG_PATH)\n",
    "if origin.shape[2] == 4:\n",
    "    origin = cv2.cvtColor(origin, cv2.COLOR_RGBA2RGB)\n",
    "origin = origin.astype(np.float)\n",
    "origin /= np.max(origin)\n",
    "plt.imshow(origin)\n",
    "plt.show()\n",
    "img = cv2.resize(origin, (int(hidden_dim/(3**0.5)), int(hidden_dim/(3**0.5))))\n",
    "#H*W*C=Parameter数になるよう（大体）に圧縮\n",
    "img = cv2.resize(img, (origin.shape[1], origin.shape[0]))\n",
    "img /= np.max(img)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "s = cv2.resize(s, (origin.shape[1], origin.shape[0]))\n",
    "plt.imshow(s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rmse = (np.sum((img-origin)**2)/(origin.shape[0]*origin.shape[1]))**0.5\n",
    "print('img_rmse:\\t', img_rmse)\n",
    "s_rmse = (np.sum((s-origin)**2)/(origin.shape[0]*origin.shape[1]))**0.5\n",
    "print('model_rmse:\\t', s_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-quantum",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COIN",
   "language": "python",
   "name": "coin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
